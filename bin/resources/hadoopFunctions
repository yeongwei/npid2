#!/bin/bash

## ==========================================================
## 1. Functions related to Hadoop
## 2. Adapted from bootstraps.sh
## ==========================================================

##
##
##
nn_service() {
	cmd=$1
	NAMENODES=$(${HADOOP_PREFIX}/bin/hdfs getconf -namenodes)
	
	log DEBUG "GYMPB0000I: ${cmd} namenodes on [${NAMENODES}]"
	
	"${HADOOP_PREFIX}/sbin/hadoop-daemon.sh" \
		--config "${HADOOP_CONF_DIR}" \
		--hostnames "${NAMENODES}" \
		--script "${HADOOP_PREFIX}/bin/hdfs" ${cmd} namenode 
	  
	SECONDARY_NAMENODES=$(${HADOOP_PREFIX}/bin/hdfs getconf -secondarynamenodes 2>/dev/null)
	
	if [ -n "${SECONDARY_NAMENODES}" ]; then
		log DEBUG "GYMPB0000I: ${cmd} secondary namenodes [${SECONDARY_NAMENODES}]"
	
	  	"${HADOOP_PREFIX}/sbin/hadoop-daemon.sh" \
			--config "${HADOOP_CONF_DIR}" \
			--hostnames "${SECONDARY_NAMENODES}" \
			--script "${HADOOP_PREFIX}/bin/hdfs" $cmd secondarynamenode
	  
		# if [ ${WAIT_PID} -eq 0 ]; then
		if [ -z "`getPID ${HADOOP_APP_NAME}`" ]; then
	  		# WAIT_PID=$(cat /tmp/hadoop-*-secondarynamenode.pid |head -1)
			# writePID ${HADOOP_APP_NAME} "`cat /tmp/hadoop-*-secondarynamenode.pid | head -1`"
			WAIT_PID=0
	  	fi
	fi
    
    	# if [ ${WAIT_PID} -eq 0 ];then
	if [ -z "`getPID ${HADOOP_APP_NAME}`" ]; then 
		# WAIT_PID=$(cat /tmp/hadoop-*-namenode.pid |head -1)
		# writePID ${HADOOP_APP_NAME}  "`cat /tmp/hadoop-*-namenode.pid | head -1`"
		WAIT_PID=0
    	fi
    
    	"${HADOOP_PREFIX}"/sbin/yarn-daemon.sh --config ${HADOOP_CONF_DIR} ${cmd} resourcemanager	
}

##
##
##
dt_service() {
	cmd=${1}
	log DEBUG "GYMPB0000D: ${cmd} datanode"

	"${HADOOP_PREFIX}/sbin/hadoop-daemon.sh" \
		--config "${HADOOP_CONF_DIR}" \
	    	--script "${HADOOP_PREFIX}/bin/hdfs" ${cmd} datanode

	log DEBUG "GYMPB0000D: ${cmd} YARN nodemanager"

	"${HADOOP_PREFIX}"/sbin/yarn-daemon.sh --config ${HADOOP_CONF_DIR} ${cmd} nodemanager	

	# if [ ${WAIT_PID} -eq 0 ];then
	if [ -z "`getPID ${HADOOP_APP_NAME}`" ]; then
		# WAIT_PID=$(cat /tmp/hadoop-*-datanode.pid |head -1)
		# writePID ${HADOOP_APP_NAME} "`cat /tmp/hadoop-*-datanode.pid | head -1`"
		WAIT_PID=0
	fi
}

##
##
##
copy_spark_jar_and_conf() {
	log DEBUG "GYMPB0000D: Waiting for recovering from safemode"
	$NPI_HOME/services/hadoop/bin/hdfs dfsadmin -safemode wait

	log DEBUG "GYMPB0000D: Creating /work/hadoop-conf folder"
	$NPI_HOME/services/hadoop/bin/hdfs dfs -mkdir -p /work/hadoop-conf

	log DEBUG "GYMPB0000D: Copying hadoop config files to hdfs"
	$NPI_HOME/services/hadoop/bin/hdfs dfs -copyFromLocal -f $NPI_HOME/services/hadoop/etc/hadoop/core-site.xml /work/hadoop-conf/core-site.xml
	$NPI_HOME/services/hadoop/bin/hdfs dfs -copyFromLocal -f $NPI_HOME/services/hadoop/etc/hadoop/hdfs-site.xml /work/hadoop-conf/hdfs-site.xml
	$NPI_HOME/services/hadoop/bin/hdfs dfs -copyFromLocal -f $NPI_HOME/services/hadoop/etc/hadoop/yarn-site.xml /work/hadoop-conf/yarn-site.xml
	$NPI_HOME/services/hadoop/bin/hdfs dfs -copyFromLocal -f $NPI_HOME/services/hadoop/etc/hadoop/spark-defaults.conf /work/hadoop-conf/spark-defaults.conf

	log DEBUG "Creating /work/spark-lib folder"
	$NPI_HOME/services/hadoop/bin/hdfs dfs -mkdir -p /work/spark-lib

	log DEBUG "Copying spark assembly jar to hdfs"
	$NPI_HOME/services/hadoop/bin/hdfs dfs -copyFromLocal -f $NPI_HOME/services/spark/lib/spark-assembly-*.jar /work/spark-lib/spark-assembly-hadoop.jar
}

##
## Comment:
##	1. Wrapper for calling initHadoopNameNode.sh
## Pending:
##	1. Need to add ${USER_CONFIG} into npi_env.sh
## 
initHadoopNameNode() {
	if [ "${USER_CONFIG}" != "true" ]; then
		if [ ! -z ${NAMENODE} ] && [ "${NAMENODE}" == "true" ]; then
			export NAMENODE_HOST=${NAMENODE_HOST:-"hdfs://$(hostname):9000/"} # should this come from npi_env.sh ???
	  		(cd ${NPI_HOME} && ./bin/initHadoopNamenode.sh ${NAMENODE_HOST})
		elif [ ! -z $NAMENODE_HOST ]; then
	  		(cd ${NPI_HOME} && ./bin/initHadoopNamenode.sh ${NAMENODE_HOST})	
		fi
	fi
}

##
## Check Hadoop status via Resource Manager
##
isHadoopStarted() {
	hadoopHost=`echo ${HADOOP_NAMENODE_URL} | sed 's/\/\///' | cut -d: -f2`
	hadoopRmUrl="http://${hadoopHost}:${HADOOP_RM_PORT}/ws/v1/cluster/info"

	status=`wget -qO - ${hadoopRmUrl} | grep '"state":"STARTED"'`
	if [ -z "${status}" ]; then
		echo 0
	else
		echo 1
	fi
}

##
## Check if Hadoop has really started
##
checkHadoop() {
	isHadoopUp=0
	tryCount=${TRY_COUNT}
	
	while [ ${tryCount} -ne 0 ]; do
		status=`isHadoopStarted`
		if [ ${status} -eq 1 ]; then
			isHadoopUp=1
			break
		fi
		sleep ${SLEEP_TIME}
		tryCount=$(( ${tryCount} - 1 ))
	done

	if [ ${isHadoopUp} -eq 0 ]; then
		emergencyExit "1"  "GYMPB0000E: Hadoop is not starting."
	else
		log INFO "GYMPB0000I: Hadoop started."
	fi
}

##
## This function will soon be deprecated
##
checkHadoop2() {
	isHadoopUp=0
	tryCount=${TRY_COUNT}

	cmd="${HADOOP_PREFIX}/bin/hdfs dfsadmin -report"
	
	while [ ${tryCount} -ne 0 ]; do
		dfsAdminResult=`${cmd}`
		if [ ! -z "${dfsAdminResult}" ]; then
			isHadoopUp=1
			break			
		fi
		sleep ${SLEEP_TIME}
		tryCount=$(( ${tryCount} - 1 ))
	done

	if [ ${isHadoopUp} -eq 0 ]; then
		emergencyExit "1"  "GYMPB0000E: Hadoop is not starting."
	fi
	
	isHadoopUp=0
	tryCount=${TRY_COUNT}
	cmd="${HADOOP_PREFIX}/bin/hdfs dfs -ls ${HADOOP_NAMENODE_URL}"

	while [ ${tryCount} -ne 0 ]; do
		dfsResult=`${cmd} | grep "work"`
		if [ ! -z "${dfsResult}" ]; then
			isHadoopUp=1
			break			
		fi
		sleep ${SLEEP_TIME}
		tryCount=$(( ${tryCount} - 1 ))
	done

	if [ ${isHadoopUp} -eq 0 ]; then
		emergencyExit "1" "GYMPB0000E: Hadoop is not starting."
	fi
}

##
##
##
soureceHadoopEnv() {
	source ${NPI_HOME}/services/hadoop/etc/hadoop/hadoop-env.sh

	if [ -f ${NPI_HOME}/services/spark/conf/spark-env.sh ]; then
		source ${NPI_HOME}/services/spark/conf/spark-env.sh
	fi
}

##
##
##
startHadoop() {
	log INFO "GYMPB0000I: Starting ${HADOOP_READABLE}."
	checkDependent ${HADOOP_APP_NAME}

	soureceHadoopEnv

	export PATH=$PATH:${HADOOP_HOME}/bin

	if [ ! -z ${NAMENODE} ] && [ "${NAMENODE}" == "true" ]; then
		if [ ! -d ${NPI_HOME}/work/dfs ]; then
			log INFO "GYMPB0000I: Starting as NAMENODE and no HDFS found, formating new HDFS."
			${NPI_HOME}/services/hadoop/bin/hdfs namenode -format
		fi
	
		nn_service start

		dt_service start

		copy_spark_jar_and_conf
	else
		dt_service start
	fi

	checkHadoop
}

##
##
##
stopHadoop() {
	log INFO "GYMPB0000I: About to stop ${HADOOP_READABLE}"
	checkDependentee ${HADOOP_APP_NAME}

	soureceHadoopEnv

	# pid=`getPID ${HADOOP_APP_NAME}`
	# if [ -z "$pid" ]; then
	#	log INFO "GYMPB0000I: ${HADOOP_READABLE} was not running."
	# else
		dt_service stop
		if [ ! -z "${NAMENODE}" ]; then
			nn_service stop
		fi

	#	tryCount=${TRY_COUNT}
	#	isHadoopDown=0

	#	while [ $tryCount -ne 0 ]; do
	#		psStatus=`getPsByName ${HADOOP_READABLE}`
	#		if [ -z "${psStatus}" ]; then
	#			isHadoopDown=1
	#			break
	#		fi
	#		sleep ${SLEEP_TIME}
	#		tryCount=$(( ${tryCount} - 1 ))
	#	done
		
	#	if [ ${isHadoopDown} -eq 0 ]; then
	#		emergencyExit "1" "GYMPB0000E: ${HADOOP_READABLE} is not stopping."
	#	else
			log INFO "GYMPB0000I: ${HADOOP_READABLE} stopped."
	#		removePID ${HADOOP_APP_NAME}
	#	fi
	# fi
}

##
## Get all PID files related to Hadoop
##
getHadoopPidFiles() {
	# Sample Hadoop PID files
	# var/yarn-sherpa-resourcemanager.pid
	# var/yarn-sherpa-nodemanager.pid
	# var/hadoop-sherpa-datanode.pid
	# var/hadoop-sherpa-secondarynamenode.pid
	# var/hadoop-sherpa-namenode.pid
	hadoopPidFiles=("${VAR_DIR}/yarn-*-resourcemanager.pid" "${VAR_DIR}/yarn-*-nodemanager.pid" "${VAR_DIR}/hadoop-*-datanode.pid" "${VAR_DIR}/hadoop-*-secondarynamenode.pid" "${VAR_DIR}/hadoop-*-namenode.pid")

	fileStr=""
	for pidFile in "${hadoopPidFiles[@]}"; do
		fileStr="${fileStr} ${pidFile}"
	done

	echo ${fileStr};
}

##
## Hadoop spawns various process, therefore a single kill is not enoughh
##
killHadoop() {
	log INFO "GYMPB0000I: TODO."
}
